import React, { useEffect, useState, useRef } from "react";
import {
  View,
  Text,
  TouchableOpacity,
  StyleSheet,
  Platform,
  PermissionsAndroid,
} from "react-native";
import AudioRecord from "react-native-audio-record";
import io from "socket.io-client";
import { server_url } from "../../api/function";

import RNFS from "react-native-fs";

const VoicePage = () => {
  const [transcription, setTranscription] = useState(""); // ë³€í™˜ëœ í…ìŠ¤íŠ¸
  const [isRecording, setIsRecording] = useState(false);
  const [loading, setLoading] = useState(true);
  const [capturedImage, setCapturedImage] = useState(null);
  const socket = useRef(null); // WebSocket

  // useEffect(() => {
  //   requestPermissions(); // âœ… ì•± ì‹¤í–‰ ì‹œ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
  // }, []);

  useEffect(() => {
    fetch(`${server_url}/api/video_play?video_id=video_0001`)
      .then((response) => response.json())
      .then(() => setLoading(false))
      .catch((error) => {
        console.error("API ìš”ì²­ ì‹¤íŒ¨:", error);
        setLoading(false);
      });
  }, []);

  useEffect(() => {
    const initWebSocketAndStartRecording = async () => {
      // âœ… ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const hasPermission = await requestPermissions();
      if (!hasPermission) {
        console.log("ğŸš« ë§ˆì´í¬ ê¶Œí•œ ì—†ìŒ. ë…¹ìŒ ë¶ˆê°€.");
        return;
      }

      // âœ… WebSocket ì—°ê²°
      socket.current = io(SERVER_URL, { transports: ["websocket"] });

      socket.current.on("connect", () => {
        console.log("âœ… WebSocket Connected!");
        startRecording(); // ğŸ¤ WebSocket ì—°ê²° í›„ ë…¹ìŒ ì‹œì‘
      });

      socket.current.on("result_data", (data) => {
        console.log("ğŸ“¦ ì„œë²„ ì‘ë‹µ:", data);
        setTranscription(data.text);
      });

      return () => {
        if (socket.current) {
          socket.current.disconnect();
          console.log("âŒ WebSocket Disconnected");
        }
      };
    };

    initWebSocketAndStartRecording();
  }, []);

  const startRecording = async () => {
    try {
      console.log("ğŸ¤ Start Recording...");

      // âœ… WAV í¬ë§·ìœ¼ë¡œ ì„¤ì • (PCM 16-bit, 44.1kHz, Mono)
      AudioRecord.init({
        sampleRate: 44100,
        channels: 1,
        bitsPerSample: 16, // PCM 16-bit
        wavFile: "recording.wav", // iOS/Android ëª¨ë‘ WAV ì €ì¥
      });

      AudioRecord.start();
      setIsRecording(true);

      // ğŸ¤ WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ì „ì†¡
      AudioRecord.on("data", (data) => {
        console.log("ğŸ¤ Recording Buffer Size:", data.length);

        if (data.length > 0 && socket.current) {
          socket.current.emit("audio_stream", data);
        }
      });
    } catch (error) {
      console.error("âŒ Recording Error:", error);
    }
  };

  const stopRecording = async () => {
    setIsRecording(false);
    try {
      const audioFile = await AudioRecord.stop();
      console.log("ğŸ“ ë…¹ìŒ íŒŒì¼ ì €ì¥ ìœ„ì¹˜:", audioFile);
    } catch (error) {
      console.error("âŒ ë…¹ìŒ ì¤‘ì§€ ì˜¤ë¥˜:", error);
    }
  };

  return (
    <View style={styles.container}>
      <TouchableOpacity
        style={styles.recordButton}
        onPress={isRecording ? stopRecording : startRecording}
      >
        <Text style={styles.buttonText}>
          {isRecording ? "â¹ ë…¹ìŒ ì¤‘ì§€" : "ğŸ¤ ë…¹ìŒ ì‹œì‘"}
        </Text>
      </TouchableOpacity>

      <Text style={styles.text}>ğŸ“œ ì¸ì‹ëœ ë¬¸ì¥: {transcription}</Text>
    </View>
  );
};

const styles = StyleSheet.create({
  container: { flex: 1, justifyContent: "center", alignItems: "center" },
  recordButton: { backgroundColor: "red", padding: 10, borderRadius: 5 },
  buttonText: { color: "white", fontSize: 18 },
  text: { marginTop: 20, fontSize: 16 },
});

export default VoicePage;
